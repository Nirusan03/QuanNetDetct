{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DrDoS_DNS.csv | Size: 1.99 GB\n",
      " DrDoS_DNS.csv is large. Processing in chunks...\n",
      "Processing: DrDoS_LDAP.csv | Size: 0.85 GB\n",
      "Processing: DrDoS_MSSQL.csv | Size: 1.76 GB\n",
      " DrDoS_MSSQL.csv is large. Processing in chunks...\n",
      "Processing: DrDoS_NetBIOS.csv | Size: 1.58 GB\n",
      " DrDoS_NetBIOS.csv is large. Processing in chunks...\n",
      "Processing: DrDoS_NTP.csv | Size: 0.6 GB\n",
      "Processing: DrDoS_SNMP.csv | Size: 2.02 GB\n",
      " DrDoS_SNMP.csv is large. Processing in chunks...\n",
      "Processing: DrDoS_SSDP.csv | Size: 1.17 GB\n",
      "Processing: DrDoS_UDP.csv | Size: 1.4 GB\n",
      "Processing: Syn.csv | Size: 0.59 GB\n",
      "Processing: TFTP.csv | Size: 8.66 GB\n",
      " TFTP.csv is large. Processing in chunks...\n",
      "Processing: TLS_Combined_Attacks.csv | Size: 0.01 GB\n",
      "Processing: UDPLag.csv | Size: 0.15 GB\n",
      "\n",
      " Final TLS attack dataset saved at: E:\\01-12\\TLS_Combined_Attacks2.csv\n",
      "TLS Attack Class Distribution:\n",
      " BENIGN           30450\n",
      "TFTP              1180\n",
      "DrDoS_SNMP         340\n",
      "DrDoS_DNS          327\n",
      "DrDoS_MSSQL        279\n",
      "DrDoS_NetBIOS      264\n",
      "Syn                190\n",
      "DrDoS_LDAP         125\n",
      "DrDoS_NTP           70\n",
      "DrDoS_UDP           48\n",
      "UDP-lag             36\n",
      "DrDoS_SSDP          31\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the dataset\n",
    "folder_path = \"E:\\\\01-12\"\n",
    "\n",
    "# Define TLS-related ports and Protocol (TCP = 6)\n",
    "tls_ports = [443, 993, 995, 8443]\n",
    "protocol_value = 6  # TCP\n",
    "\n",
    "# Define chunk size for large files\n",
    "chunk_size = 100000  # 100k rows per chunk\n",
    "large_file_threshold = 1.5 * (1024 ** 3)  # 1.5GB in bytes\n",
    "\n",
    "# List to store TLS traffic DataFrames\n",
    "tls_attacks_list = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if it's a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "\n",
    "        print(f\"Processing: {file_name} | Size: {round(file_size / (1024 ** 3), 2)} GB\")\n",
    "        \n",
    "        if file_size > large_file_threshold:\n",
    "            # Process large files in chunks\n",
    "            print(f\" {file_name} is large. Processing in chunks...\")\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "                tls_chunk = chunk[(chunk[' Destination Port'].isin(tls_ports))]\n",
    "                if not tls_chunk.empty:\n",
    "                    tls_attacks_list.append(tls_chunk)\n",
    "        else:\n",
    "            # Process small files normally\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            tls_attacks = df[(df[' Destination Port'].isin(tls_ports))]\n",
    "            if not tls_attacks.empty:\n",
    "                tls_attacks_list.append(tls_attacks)\n",
    "\n",
    "# Combine all extracted TLS attack data\n",
    "if tls_attacks_list:\n",
    "    combined_tls_attacks = pd.concat(tls_attacks_list, ignore_index=True)\n",
    "\n",
    "    # Save the final dataset\n",
    "    output_path = os.path.join(folder_path, \"TLS_Combined_Attacks2.csv\")\n",
    "    combined_tls_attacks.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n Final TLS attack dataset saved at: {output_path}\")\n",
    "    print(\"TLS Attack Class Distribution:\\n\", combined_tls_attacks[' Label'].value_counts())\n",
    "else:\n",
    "    print(\"\\n No TLS-based attack traffic found in any file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: LDAP.csv | Size: 0.81 GB\n",
      "Processing: MSSQL.csv | Size: 2.22 GB\n",
      " MSSQL.csv is large. Processing in chunks...\n",
      "Processing: NetBIOS.csv | Size: 1.32 GB\n",
      "Processing: Portmap.csv | Size: 0.07 GB\n",
      "Processing: Syn.csv | Size: 1.75 GB\n",
      " Syn.csv is large. Processing in chunks...\n",
      "Processing: UDP.csv | Size: 1.67 GB\n",
      " UDP.csv is large. Processing in chunks...\n",
      "Processing: UDPLag.csv | Size: 0.3 GB\n",
      "\n",
      "Final TLS attack dataset saved at: E:\\TLS_Final_Merged_Attacks.csv\n",
      "TLS Attack Class Distribution:\n",
      " BENIGN     21500\n",
      "MSSQL        355\n",
      "Syn          335\n",
      "NetBIOS      245\n",
      "LDAP         119\n",
      "UDP           59\n",
      "Portmap       20\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the new folder containing CICDDoS2019 dataset\n",
    "folder_path = \"E:\\\\03-11\"\n",
    "\n",
    "# Define TLS-related ports and Protocol (TCP = 6)\n",
    "tls_ports = [443, 993, 995, 8443]\n",
    "\n",
    "# Define chunk size for large files\n",
    "chunk_size = 100000  # 100k rows per chunk\n",
    "large_file_threshold = 1.5 * (1024 ** 3)  # 1.5GB in bytes\n",
    "\n",
    "# List to store TLS traffic DataFrames\n",
    "tls_attacks_list = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if it's a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "\n",
    "        print(f\"Processing: {file_name} | Size: {round(file_size / (1024 ** 3), 2)} GB\")\n",
    "        \n",
    "        if file_size > large_file_threshold:\n",
    "            # Process large files in chunks\n",
    "            print(f\" {file_name} is large. Processing in chunks...\")\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "                tls_chunk = chunk[(chunk[' Destination Port'].isin(tls_ports))]\n",
    "                if not tls_chunk.empty:\n",
    "                    tls_attacks_list.append(tls_chunk)\n",
    "        else:\n",
    "            # Process small files normally\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            tls_attacks = df[(df[' Destination Port'].isin(tls_ports))]\n",
    "            if not tls_attacks.empty:\n",
    "                tls_attacks_list.append(tls_attacks)\n",
    "\n",
    "# Combine all extracted TLS attack data\n",
    "if tls_attacks_list:\n",
    "    combined_tls_attacks = pd.concat(tls_attacks_list, ignore_index=True)\n",
    "\n",
    "    # Merge with previous dataset (TLS_Combined_Attacks2.csv)\n",
    "    prev_dataset_path = \"E:\\\\TLS_Combined_Attacks2.csv\"\n",
    "    if os.path.exists(prev_dataset_path):\n",
    "        prev_data = pd.read_csv(prev_dataset_path, low_memory=False)\n",
    "        combined_tls_attacks = pd.concat([prev_data, combined_tls_attacks], ignore_index=True)\n",
    "        print(\"Merged with previous dataset: TLS_Combined_Attacks2.csv\")\n",
    "\n",
    "    # Save the final merged dataset\n",
    "    output_path = \"E:\\\\TLS_Final_Merged_Attacks.csv\"\n",
    "    combined_tls_attacks.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nFinal TLS attack dataset saved at: {output_path}\")\n",
    "    print(\"TLS Attack Class Distribution:\\n\", combined_tls_attacks[' Label'].value_counts())\n",
    "else:\n",
    "    print(\"\\n No TLS-based attack traffic found in any file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing non-numeric columns: ['Flow ID', ' Source IP', ' Destination IP', ' Timestamp', ' Label']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Apply SMOTE\u001b[39;00m\n\u001b[0;32m     17\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert back to DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m balanced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_resampled, columns\u001b[38;5;241m=\u001b[39mX_numeric\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nirusan03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"E:\\\\\\\\01-12\\\\TLS_Combined_Attacks2.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"Removing non-numeric columns: {list(non_numeric_cols)}\")\n",
    "\n",
    "# Separate numeric and categorical data\n",
    "X_numeric = df.drop(columns=non_numeric_cols)  # Keep only numeric features\n",
    "y = df[' Label']  # Target variable\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_numeric, y)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "balanced_df = pd.DataFrame(X_resampled, columns=X_numeric.columns)\n",
    "balanced_df[' Label'] = y_resampled  # Reattach target variable\n",
    "\n",
    "# Save the balanced dataset\n",
    "output_path = \"E:\\\\TLS_Balanced_Attacks.csv\"\n",
    "balanced_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Balanced TLS dataset saved successfully as 'TLS_Balanced_Attacks.csv'\")\n",
    "print(balanced_df[' Label'].value_counts())  # Check new class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TLS-related attacks found: 559\n",
      "443     469\n",
      "995      52\n",
      "8443     20\n",
      "993      18\n",
      "Name:  Destination Port, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a sample of the dataset (avoids memory issues)\n",
    "file_path = \"E:\\\\01-12\\\\DrDoS_LDAP.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Check how many records use TLS-related ports\n",
    "tls_ports = [443, 993, 995, 8443]\n",
    "tls_traffic = df[df[' Destination Port'].isin(tls_ports)]\n",
    "\n",
    "print(\"Total TLS-related attacks found:\", tls_traffic.shape[0])\n",
    "print(tls_traffic[' Destination Port'].value_counts())  # See which TLS ports are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TLS-related attacks found: 193\n",
      "443     130\n",
      "993      30\n",
      "995      20\n",
      "8443     13\n",
      "Name:  Destination Port, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a sample of the dataset (avoids memory issues)\n",
    "file_path = \"E:\\\\CSV-01-12\\\\01-12\\\\Syn.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Check how many records use TLS-related ports\n",
    "tls_ports = [443, 993, 995, 8443]\n",
    "tls_traffic = df[df[' Destination Port'].isin(tls_ports)]\n",
    "\n",
    "print(\"Total TLS-related attacks found:\", tls_traffic.shape[0])\n",
    "print(tls_traffic[' Destination Port'].value_counts())  # See which TLS ports are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TLS-related attacks found: 640\n",
      "443     597\n",
      "8443     43\n",
      "Name:  Destination Port, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a sample of the dataset (avoids memory issues)\n",
    "file_path = \"E:\\\\CSV-01-12\\\\01-12\\\\DrDoS_UDP.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Check how many records use TLS-related ports\n",
    "tls_ports = [443, 993, 995, 8443]\n",
    "tls_traffic = df[df[' Destination Port'].isin(tls_ports)]\n",
    "\n",
    "print(\"Total TLS-related attacks found:\", tls_traffic.shape[0])\n",
    "print(tls_traffic[' Destination Port'].value_counts())  # See which TLS ports are used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
